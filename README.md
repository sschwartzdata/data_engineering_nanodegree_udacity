# data_engineering_nanodegree_udacity
This repository contains the five mandatory projects for completing Udacity's Nanodegree in Data Engineering and the final Capstone. 

## Project One: Data Modeling with Postgres
### Description:
In this project, you'll apply what you've learned on data modeling with Postgres and build an ETL pipeline using Python. To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.
### Skills and Programs Used:
- Python
- SQL
- PostgresQL
- Jupyter Notebooks

## Project Two: Data Modeling with Apache Cassandra
### Description:
In this project, you'll apply what you've learned on data modeling with Apache Cassandra and complete an ETL pipeline using Python. To complete the project, you will need to model your data by creating tables in Apache Cassandra to run queries. You are provided with part of the ETL pipeline that transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables.

### Skills and Programs Used:
- Python
- NoSQL (Cassandra Query Language)
- Cassandra
- Jupyter Notebooks

## Project Three: Data Warehousing with Redshift
### Description:
In this project, you'll apply what you've learned on data warehouses and AWS to build an ETL pipeline for a database hosted on Redshift. To complete the project, you will need to load data from S3 to staging tables on Redshift and execute SQL statements that create the analytics tables from these staging tables.

### Skills and Programs Used:
- Python
- SQL
- Redshift
- S3
- JSON
- Jupyter Notebooks

## Project Four: Data Lakes and Spark
### Description:
In this project, you'll apply what you've learned on Spark and data lakes to build an ETL pipeline for a data lake hosted on S3. To complete the project, you will need to load data from S3, process the data into analytics tables using Spark, and load them back into S3. You'll deploy this Spark process on a cluster using AWS.

### Skills and Programs Used:
- Python
- SQL
- EMR
- S3
- JSON
- Jupyter Notebooks
